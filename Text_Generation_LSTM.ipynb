{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en',disable=['parser','tagger','ner'])\n",
    "def readfile(src):\n",
    "    with open(src) as  rd:\n",
    "        \n",
    "        ret=rd.read()\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated=readfile('F:/pycharmprojects/Sir Allama Iqbal 2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmvpunc(src):\n",
    "    tk= [token.text.lower() for token in nlp(src) if token.text not in [nlp.Defaults.stop_words]]\n",
    "    return [token for token in tk if token not in '\",.)\\n\\n\":;...... ... (   -']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens=rmvpunc(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3234"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens=len(ntokens)\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlen=30+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sequence=[]\n",
    "for i in range(trainlen,total_tokens):\n",
    "    appnd=ntokens[i-trainlen:i]\n",
    "    raw_sequence.append(appnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with the defeat of the sikhs in punjab by the british army western missionaries wasted no time in establishing centres of learning in sialkot one of these the scotch mission college'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(raw_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the defeat of the sikhs in punjab by the british army western missionaries wasted no time in establishing centres of learning in sialkot one of these the scotch mission college founded'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(raw_sequence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(raw_sequence)\n",
    "seq = tokenizer.texts_to_sequences(raw_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46   with\n",
      "1   the\n",
      "390   defeat\n",
      "2   of\n",
      "1   the\n",
      "1141   sikhs\n",
      "3   in\n",
      "387   punjab\n",
      "16   by\n",
      "1   the\n",
      "71   british\n",
      "1139   army\n",
      "189   western\n",
      "1136   missionaries\n",
      "1135   wasted\n",
      "93   no\n",
      "187   time\n",
      "3   in\n",
      "1134   establishing\n",
      "1133   centres\n",
      "2   of\n",
      "1132   learning\n",
      "3   in\n",
      "1130   sialkot\n",
      "56   one\n",
      "2   of\n",
      "92   these\n",
      "1   the\n",
      "186   scotch\n",
      "185   mission\n",
      "191   college\n"
     ]
    }
   ],
   "source": [
    "for i in seq[0]:\n",
    "    print(i,\" \",tokenizer.index_word[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabsize=len(tokenizer.word_counts)\n",
    "vocabsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_seq=np.array(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  46,    1,  390, ...,  186,  185,  191],\n",
       "       [   1,  390,    2, ...,  185,  191,  391],\n",
       "       [ 390,    2,    1, ...,  191,  391,    3],\n",
       "       ...,\n",
       "       [1126,    2, 1127, ...,  389, 1142,    2],\n",
       "       [   2, 1127,   23, ..., 1142,    2,    1],\n",
       "       [1127,   23,    1, ...,    2,    1,  127]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3203, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=final_seq[:,:-1]\n",
    "y=final_seq[:,-1]\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3203, 1143)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= to_categorical(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqln=x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "call = EarlyStopping(monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call= EarlyStopping(monitor='loss',patience=2)\n",
    "def createmodel(vsize,trainl,seqlen):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding((vsize+1),trainl,input_length=seqlen))\n",
    "    model.add(LSTM(200,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(400 , activation='relu'))\n",
    "    model.add(Dense(vsize+1,activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 31)            35433     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 30, 200)           185600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                50200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               40400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1143)              458343    \n",
      "=================================================================\n",
      "Total params: 777,626\n",
      "Trainable params: 777,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel=createmodel(vocabsize,trainlen,seqln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2562 samples, validate on 641 samples\n",
      "Epoch 1/100\n",
      "2562/2562 [==============================] - 16s 6ms/sample - loss: 6.6503 - accuracy: 0.0757 - val_loss: 6.3800 - val_accuracy: 0.0811\n",
      "Epoch 2/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.9557 - accuracy: 0.0800 - val_loss: 6.6942 - val_accuracy: 0.0811\n",
      "Epoch 3/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.8592 - accuracy: 0.0800 - val_loss: 6.9231 - val_accuracy: 0.0811\n",
      "Epoch 4/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.8448 - accuracy: 0.0800 - val_loss: 6.9501 - val_accuracy: 0.0811\n",
      "Epoch 5/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.8254 - accuracy: 0.0800 - val_loss: 7.0314 - val_accuracy: 0.0811\n",
      "Epoch 6/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.8271 - accuracy: 0.0800 - val_loss: 7.0873 - val_accuracy: 0.0811\n",
      "Epoch 7/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.8070 - accuracy: 0.0800 - val_loss: 7.1394 - val_accuracy: 0.0811\n",
      "Epoch 8/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.7963 - accuracy: 0.0648 - val_loss: 7.1396 - val_accuracy: 0.0811\n",
      "Epoch 9/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.7602 - accuracy: 0.0800 - val_loss: 7.1038 - val_accuracy: 0.0811\n",
      "Epoch 10/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.6811 - accuracy: 0.0800 - val_loss: 7.1627 - val_accuracy: 0.0811\n",
      "Epoch 11/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.5855 - accuracy: 0.0667 - val_loss: 6.9439 - val_accuracy: 0.0811\n",
      "Epoch 12/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.5202 - accuracy: 0.0827 - val_loss: 7.4517 - val_accuracy: 0.0811\n",
      "Epoch 13/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.5026 - accuracy: 0.0441 - val_loss: 7.3583 - val_accuracy: 0.0811\n",
      "Epoch 14/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.3996 - accuracy: 0.0859 - val_loss: 7.5236 - val_accuracy: 0.0811\n",
      "Epoch 15/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.3112 - accuracy: 0.0886 - val_loss: 7.6601 - val_accuracy: 0.0811\n",
      "Epoch 16/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.2161 - accuracy: 0.0925 - val_loss: 7.8900 - val_accuracy: 0.0811\n",
      "Epoch 17/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.1461 - accuracy: 0.0980 - val_loss: 8.5049 - val_accuracy: 0.0764\n",
      "Epoch 18/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.0782 - accuracy: 0.0988 - val_loss: 8.9031 - val_accuracy: 0.0889\n",
      "Epoch 19/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 5.0486 - accuracy: 0.0913 - val_loss: 9.2619 - val_accuracy: 0.0920\n",
      "Epoch 20/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.9862 - accuracy: 0.1066 - val_loss: 9.8462 - val_accuracy: 0.0842\n",
      "Epoch 21/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.9702 - accuracy: 0.1019 - val_loss: 9.5712 - val_accuracy: 0.0811\n",
      "Epoch 22/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.9437 - accuracy: 0.1003 - val_loss: 10.1803 - val_accuracy: 0.0905\n",
      "Epoch 23/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.9167 - accuracy: 0.1155 - val_loss: 9.5405 - val_accuracy: 0.0780\n",
      "Epoch 24/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.8593 - accuracy: 0.1155 - val_loss: 10.1941 - val_accuracy: 0.0889\n",
      "Epoch 25/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.8196 - accuracy: 0.1163 - val_loss: 10.4540 - val_accuracy: 0.0905\n",
      "Epoch 26/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.7735 - accuracy: 0.1151 - val_loss: 10.2194 - val_accuracy: 0.0811\n",
      "Epoch 27/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.7392 - accuracy: 0.1210 - val_loss: 10.6985 - val_accuracy: 0.0889\n",
      "Epoch 28/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.7121 - accuracy: 0.1183 - val_loss: 11.1032 - val_accuracy: 0.0905\n",
      "Epoch 29/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.6605 - accuracy: 0.1261 - val_loss: 11.2354 - val_accuracy: 0.0842\n",
      "Epoch 30/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.7219 - accuracy: 0.1183 - val_loss: 11.0789 - val_accuracy: 0.0889\n",
      "Epoch 31/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.6169 - accuracy: 0.1276 - val_loss: 11.2666 - val_accuracy: 0.0874\n",
      "Epoch 32/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.5533 - accuracy: 0.1319 - val_loss: 11.8297 - val_accuracy: 0.0920\n",
      "Epoch 33/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.5356 - accuracy: 0.1245 - val_loss: 11.9082 - val_accuracy: 0.0920\n",
      "Epoch 34/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.5102 - accuracy: 0.1425 - val_loss: 12.0564 - val_accuracy: 0.0874\n",
      "Epoch 35/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.4460 - accuracy: 0.1362 - val_loss: 12.3434 - val_accuracy: 0.0827\n",
      "Epoch 36/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.3959 - accuracy: 0.1413 - val_loss: 12.2654 - val_accuracy: 0.0827\n",
      "Epoch 37/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.3445 - accuracy: 0.1421 - val_loss: 12.6468 - val_accuracy: 0.0764\n",
      "Epoch 38/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.3586 - accuracy: 0.1382 - val_loss: 12.5143 - val_accuracy: 0.0796\n",
      "Epoch 39/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.4658 - accuracy: 0.1292 - val_loss: 12.2881 - val_accuracy: 0.0764\n",
      "Epoch 40/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.3146 - accuracy: 0.1417 - val_loss: 12.4137 - val_accuracy: 0.0842\n",
      "Epoch 41/100\n",
      "2562/2562 [==============================] - 6s 2ms/sample - loss: 4.2372 - accuracy: 0.1499 - val_loss: 12.6156 - val_accuracy: 0.0733\n",
      "Epoch 42/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.1902 - accuracy: 0.1448 - val_loss: 12.9010 - val_accuracy: 0.0889\n",
      "Epoch 43/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.1610 - accuracy: 0.1479 - val_loss: 13.0654 - val_accuracy: 0.0780\n",
      "Epoch 44/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.1253 - accuracy: 0.1534 - val_loss: 13.5416 - val_accuracy: 0.0764\n",
      "Epoch 45/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.2815 - accuracy: 0.1487 - val_loss: 12.8726 - val_accuracy: 0.0920\n",
      "Epoch 46/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.2135 - accuracy: 0.1413 - val_loss: 12.6389 - val_accuracy: 0.0905\n",
      "Epoch 47/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.1669 - accuracy: 0.1354 - val_loss: 12.5783 - val_accuracy: 0.0655\n",
      "Epoch 48/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.1256 - accuracy: 0.1577 - val_loss: 13.1827 - val_accuracy: 0.0920\n",
      "Epoch 49/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.0915 - accuracy: 0.1495 - val_loss: 13.2471 - val_accuracy: 0.0827\n",
      "Epoch 50/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.9817 - accuracy: 0.1569 - val_loss: 13.4938 - val_accuracy: 0.0749\n",
      "Epoch 51/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 4.0308 - accuracy: 0.1577 - val_loss: 13.5585 - val_accuracy: 0.0811\n",
      "Epoch 52/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.9871 - accuracy: 0.1518 - val_loss: 13.6890 - val_accuracy: 0.0811\n",
      "Epoch 53/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.9740 - accuracy: 0.1553 - val_loss: 13.7896 - val_accuracy: 0.0702\n",
      "Epoch 54/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.8821 - accuracy: 0.1639 - val_loss: 14.0872 - val_accuracy: 0.0796\n",
      "Epoch 55/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.8873 - accuracy: 0.1499 - val_loss: 13.9676 - val_accuracy: 0.0811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.8335 - accuracy: 0.1616 - val_loss: 14.3616 - val_accuracy: 0.0655\n",
      "Epoch 57/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.7972 - accuracy: 0.1624 - val_loss: 14.5246 - val_accuracy: 0.0624\n",
      "Epoch 58/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.8158 - accuracy: 0.1604 - val_loss: 14.3094 - val_accuracy: 0.0671\n",
      "Epoch 59/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.7810 - accuracy: 0.1593 - val_loss: 14.6113 - val_accuracy: 0.0718\n",
      "Epoch 60/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.7169 - accuracy: 0.1678 - val_loss: 14.7268 - val_accuracy: 0.0655\n",
      "Epoch 61/100\n",
      "2562/2562 [==============================] - 6s 2ms/sample - loss: 3.6816 - accuracy: 0.1772 - val_loss: 15.0521 - val_accuracy: 0.0764\n",
      "Epoch 62/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.6719 - accuracy: 0.1784 - val_loss: 14.9722 - val_accuracy: 0.0671\n",
      "Epoch 63/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.6185 - accuracy: 0.1811 - val_loss: 15.1215 - val_accuracy: 0.0640\n",
      "Epoch 64/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.6150 - accuracy: 0.1893 - val_loss: 15.6646 - val_accuracy: 0.0811\n",
      "Epoch 65/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.6273 - accuracy: 0.1819 - val_loss: 15.4176 - val_accuracy: 0.0686\n",
      "Epoch 66/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.5837 - accuracy: 0.1948 - val_loss: 15.1569 - val_accuracy: 0.0749\n",
      "Epoch 67/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.5156 - accuracy: 0.1928 - val_loss: 15.5371 - val_accuracy: 0.0733\n",
      "Epoch 68/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.5238 - accuracy: 0.1967 - val_loss: 15.4903 - val_accuracy: 0.0702\n",
      "Epoch 69/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.5011 - accuracy: 0.1924 - val_loss: 15.4663 - val_accuracy: 0.0640\n",
      "Epoch 70/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.4840 - accuracy: 0.1893 - val_loss: 15.5245 - val_accuracy: 0.0733\n",
      "Epoch 71/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.4249 - accuracy: 0.2123 - val_loss: 15.9731 - val_accuracy: 0.0640\n",
      "Epoch 72/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.4970 - accuracy: 0.1936 - val_loss: 15.9506 - val_accuracy: 0.0577\n",
      "Epoch 73/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.4116 - accuracy: 0.2194 - val_loss: 16.1448 - val_accuracy: 0.0749\n",
      "Epoch 74/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.3664 - accuracy: 0.2119 - val_loss: 15.9900 - val_accuracy: 0.0484\n",
      "Epoch 75/100\n",
      "2562/2562 [==============================] - 6s 2ms/sample - loss: 3.3986 - accuracy: 0.2041 - val_loss: 16.1249 - val_accuracy: 0.0671\n",
      "Epoch 76/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.3792 - accuracy: 0.2139 - val_loss: 16.1750 - val_accuracy: 0.0733\n",
      "Epoch 77/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.2630 - accuracy: 0.2201 - val_loss: 16.7412 - val_accuracy: 0.0733\n",
      "Epoch 78/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.2853 - accuracy: 0.2319 - val_loss: 16.8973 - val_accuracy: 0.0624\n",
      "Epoch 79/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.3923 - accuracy: 0.2061 - val_loss: 16.4256 - val_accuracy: 0.0577\n",
      "Epoch 80/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.2522 - accuracy: 0.2322 - val_loss: 16.4950 - val_accuracy: 0.0593\n",
      "Epoch 81/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.2371 - accuracy: 0.2361 - val_loss: 16.7337 - val_accuracy: 0.0671\n",
      "Epoch 82/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1669 - accuracy: 0.2334 - val_loss: 17.4176 - val_accuracy: 0.0640\n",
      "Epoch 83/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1911 - accuracy: 0.2369 - val_loss: 17.4496 - val_accuracy: 0.0593\n",
      "Epoch 84/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1412 - accuracy: 0.2400 - val_loss: 17.7637 - val_accuracy: 0.0499\n",
      "Epoch 85/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1178 - accuracy: 0.2447 - val_loss: 17.9899 - val_accuracy: 0.0655\n",
      "Epoch 86/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1697 - accuracy: 0.2397 - val_loss: 17.9097 - val_accuracy: 0.0562\n",
      "Epoch 87/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1860 - accuracy: 0.2213 - val_loss: 17.4501 - val_accuracy: 0.0718\n",
      "Epoch 88/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.1031 - accuracy: 0.2549 - val_loss: 17.8276 - val_accuracy: 0.0608\n",
      "Epoch 89/100\n",
      "2562/2562 [==============================] - 5s 2ms/sample - loss: 3.0456 - accuracy: 0.2560 - val_loss: 17.9581 - val_accuracy: 0.0640\n",
      "Epoch 90/100\n",
      "1408/2562 [===============>..............] - ETA: 2s - loss: 3.0366 - accuracy: 0.2351"
     ]
    }
   ],
   "source": [
    "\n",
    "mymodel.fit(x,y,epochs=100,verbose=1,validation_split=0.2,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mymodel.save('iq.h5')\n",
    "plt.figure(figsize=(10,6))\n",
    "import seaborn as sns\n",
    "df=pd.DataFrame(mymodel.history.history)\n",
    "plt.plot(df['accuracy'],label='accu')\n",
    "plt.plot(df['val_accuracy'],label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "text=['missionaries wasted no time in establishing centres of learning in']\n",
    "encoded=tokenizer.texts_to_sequences(text)[0]\n",
    "print(encoded)\n",
    "padded=pad_sequences([encoded],maxlen=25,truncating='pre') \n",
    "print(padded)\n",
    "predind=mymodel.predict_classes([padded],verbose=0)[0]\n",
    "print(tokenizer.index_word[predind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def generatetext(model,tokenizer,seq_len,text,num_gen_words):\n",
    "    output_text=[]\n",
    "    inputt=text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded=tokenizer.texts_to_sequences(inputt)[0]\n",
    "        print(encoded)\n",
    "        padded=pad_sequences([encoded],maxlen=seq_len,truncating='pre')\n",
    "        \n",
    "        predind=model.predict_classes([padded],verbose=0)[0]\n",
    "        \n",
    "        word=tokenizer.index_word[predind]\n",
    "        inputt.append(word)\n",
    "        output_text.append(word)\n",
    "    text.append(output_text)\n",
    "    print(str(text))\n",
    "text=['missionaries wasted no time in establishing centres of learning in']\n",
    "generatetext(mymodel,tokenizer,seqln,text,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
